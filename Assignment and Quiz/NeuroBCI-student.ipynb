{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuroBCI-student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ram8qve2q33"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os \n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXaPfZGO_4j8"
      },
      "source": [
        "This is an image from Yu et al. (2011) IEEE Circuits and Systems that gives us a good visualization of the approach on spike detection to spike classification via dimensionality reduction.\n",
        "![](https://www.researchgate.net/profile/Xiangyu_Li4/publication/260619153/figure/fig1/AS:339519564992512@1457959180390/Hebbian-eigenfilter-based-spike-sorting-algorithm.png)\n",
        "\n",
        "Below, we will be working on simlated spiking data to start with. The simulated data that we will be working with is described in depth in this paper: https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/Publications/martinezJNM09.pdf\n",
        "\n",
        "_Martinez J, Pedreira C, Ison MJ, Quian Quiroga R. Realistic simulation of extracellular recordings. J Neurosci Methods. 2009 Nov 15;184(2):285-93. doi: 10.1016/j.jneumeth.2009.08.017. Epub 2009 Aug 22. PMID: 19703490._\n",
        "\n",
        "**The general workflow**\n",
        "1. First, we need to preprocess the raw voltage recordings from ECoG or EEG (most likely, you will do spike sorting with ECoG as it will give the best spiking signal). This include filtering the frequencies and removing noise from the signal.\n",
        "2. Next, with a cleaned signal, we need to detect peaks that represent neuronal spiking. Various algorithms have been described (in the figure above, NEO refers to nonlinear energy operator, a method of using energy to detect peaks). In the below code, we will use the spiking peaks that have already been given to us with the data. \n",
        "Usually, after a peak detection step, the waveforms of the peaks are acquired by obtaining the signal around the peak, and then aligning the waveforms so that the peak appear in the middle (this is called peak alignment). \n",
        "3. After waveform extraction, the peaks are represented in a d dimensional space that represents the signal around the time of the peak. This is where we want to be able to perform unsupervised clustering on the various waveforms. \n",
        "Usually, a dimensionality reduction algorithm is applied, such as PCA, to represent the waveforms in a lower dimensional space. After that, K-means clustering or other unsupervised clustering algorithms are implemented in order to perform waveform classification. \n",
        "With classification complete, we have a general idea of a neuron's behavior based upon the shape of the peak.\n",
        "4. After classification, we can then look at other metrics of a neuron's behavior. Such as spike trains or frequency of excitation. We can examine how these signals change with certain tasks such as controlling a cursor on a computer. We can also examine how these signals are dysfunctional in disease like epilepsy. This is the basis of how brain computer interface (BCI) works, and how spike sorting comes into play. \n",
        "\n",
        "Here is a whole article on more details of spike sorting: http://www.scholarpedia.org/article/Spike_sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klPL4lWHNcdM"
      },
      "source": [
        "# Spike Sorting on Simulated Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIRyQqGRlEuS"
      },
      "source": [
        "## Data Preparation \n",
        "First, let us download simulated extracellular recordings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB5ZUjI4cPDE"
      },
      "source": [
        "import scipy.io\n",
        "import scipy.cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ed5Jv_IcPii"
      },
      "source": [
        "# download data\n",
        "!wget https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/simulations/simulation-1.mat\n",
        "!wget https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/simulations/simulation-2.mat\n",
        "# !wget https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/simulations/simulation-3.mat\n",
        "# !wget https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/simulations/simulation-4.mat\n",
        "# !wget https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/simulations/simulation-5.mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8q5yW6O9Bf"
      },
      "source": [
        "# extract signals and class\n",
        "sim1 = {k:v for k,v in scipy.io.loadmat('simulation-2').items()}\n",
        "signal= np.squeeze(sim1[\"data\"])\n",
        "\n",
        "spikeclass = sim1[\"spike_class\"][0][0][0]\n",
        "spiketimes = np.squeeze(sim1[\"spike_times\"][0][0])\n",
        "\n",
        "print('Number of Spikes in each Class:')\n",
        "for i, c in zip([0,1,2],np.bincount(spikeclass)):\n",
        "  print(i, ':', c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAZB4_C6Wgvl"
      },
      "source": [
        "The simulated signals are 2 minute recordings, with a sampling rate of 24kHz. \n",
        "\n",
        "There are 2 single unit class of neurons that we want to classify, mixed in with 1 multiunit class. The detection threshold is 24mV.\n",
        "\n",
        "Let us visualize these recordings below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkkkct_FIXR6"
      },
      "source": [
        "# Visualize the signal\n",
        "fs = 24000.0 # 24kHz sampling\n",
        "plt.figure(figsize = (20,10))\n",
        "seconds = np.arange(signal.shape[0])/fs\n",
        "plt.plot(seconds, signal, label = 'signal')\n",
        "plt.scatter(spiketimes/fs, np.ones(spiketimes.shape)*-100, c = 'r', s = 2, label = 'spiking')\n",
        "plt.axhline(24, linestyle =':', c = 'g', label = 'detection threshold'); plt.legend(); \n",
        "plt.title('Extracellular Recordings of Neurons'); plt.xlim([1,5]); \n",
        "plt.xlabel('Time (s)'); plt.ylabel('Voltage (uA)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9adwtfDVXDCC"
      },
      "source": [
        "### Preprocessing\n",
        "As a preprocessing step, we want to perform filtering on the raw recorded signals. Then, we want to be able to extract spikes in order to classify the spikes. To prepare out features, we will perform peak detection to identify spiking in our recordings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhPkwV9zTpyT"
      },
      "source": [
        "# First, apply a bandpass filter to remove unnecessary frequencies in our raw recording.\n",
        "from scipy.signal import butter, lfilter\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "fs = 24000.0 # sampling frequency\n",
        "lowcut = 30.0 # low frequency cutoff\n",
        "highcut = 3000.0 # high frequency cutoff\n",
        "\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.plot(signal, label = 'noisy signal', alpha = .7)\n",
        "# filter the signal\n",
        "signalfilt = butter_bandpass_filter(signal, lowcut, highcut, fs, order=6)\n",
        "plt.plot(signalfilt, label='Filtered signal', alpha = .7)\n",
        "plt.axhline(24, linestyle =':', c = 'g', label = 'detection threshold');\n",
        "plt.axis('tight'); plt.legend(loc='upper left')\n",
        "plt.xlim([0,10000]); plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWJEWzh9W-g7"
      },
      "source": [
        "# Next, we will perform peak detection. \n",
        "# There are various algorithms out there to detect peaks, but in our case, we will perform detection using scipy's algorithm.\n",
        "from scipy.signal import find_peaks, peak_prominences\n",
        "thres = 24\n",
        "\n",
        "pks = find_peaks(signalfilt, height = thres, distance = 60)[0]\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.plot(signalfilt, label = 'signal')\n",
        "plt.scatter(pks, signalfilt[pks], c = 'r', label = 'detected peaks')\n",
        "plt.scatter(spiketimes, np.ones(spiketimes.shape)*-100, c = 'c', s = 5, label = 'given spike times')\n",
        "plt.xlim([100,100000]); plt.legend(); plt.title('Detected Peaks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7AgW2NaWzUX"
      },
      "source": [
        "# From our detected peaks, we then want to extract waveforms from detected peaks\n",
        "# These will be the signals around the peaks that we detected. These signals will become our features in classification.\n",
        "waveforms = list()\n",
        "leftlim = 30\n",
        "rightlim = 30\n",
        "for p in pks:\n",
        "  if p < leftlim:\n",
        "    continue;\n",
        "  if (len(signalfilt) - p) < rightlim: \n",
        "    continue;\n",
        "  waveforms.append(signalfilt[p-leftlim:p+rightlim])\n",
        "\n",
        "## plot the waveforms to visualize the data you will be working with\n",
        "## CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febGhwaWZHU5"
      },
      "source": [
        "# Spike Sorting: Training\n",
        "Since this neural signal is simulated, we already have prior knowledge of assigned neural classes. In this example, we will use this knowledge to validate our classification method. In practice, you do not know these underlying classes and will have to performed un-supervised classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bTcq5obY1Zx"
      },
      "source": [
        "waveforms = list()\n",
        "leftlim = 20\n",
        "rightlim = 50\n",
        "\n",
        "plt.figure(); plt.title('pre-aligned');\n",
        "for p in spiketimes:\n",
        "  testwave = signalfilt[p-2:p+70]\n",
        "  pk = find_peaks(testwave)[0]\n",
        "  plt.plot(testwave)\n",
        "  pk = pk[np.argmax(testwave[pk])]\n",
        "  waveforms.append(signalfilt[p+pk-leftlim:p+pk+rightlim])\n",
        "waveforms = np.vstack(waveforms)\n",
        "plt.show();  plt.figure(); \n",
        "plt.plot(waveforms.T); \n",
        "plt.title('Spike Waveforms: Aligned');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDwBCzqWmfgv"
      },
      "source": [
        "Now we have extracted our aligned waveforms and put them in `waveforms_df`.\n",
        "\n",
        "The classes are represented by the column `class`.\n",
        "\n",
        "Below, plot out all the spikes within each class, as well as the average spike waveform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw6LlI3Xba0V"
      },
      "source": [
        "# Now plot each of the classes.\n",
        "# Spike classes are in spikeclass and there are 3 classes (2 corresponding to individual neurons, one class a mix of multiple neurons)\n",
        "waveforms_df = pd.DataFrame(waveforms)\n",
        "waveforms_df['class'] = spikeclass\n",
        "\n",
        "## CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-2YPoqYu4F"
      },
      "source": [
        "## Dimensionality reduction and clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcrVq_TSm5rG"
      },
      "source": [
        "First, let's split our data into training and test sets. You can use the train_test_split function from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhHYmPbCeT-t"
      },
      "source": [
        "# recall your training data waveforms are in waveforms, and your labels are in spikeclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## CODE HERE\n",
        "X_train, X_test, y_train, y_test = \n",
        "\n",
        "## END CODE \n",
        "\n",
        "num_train, dims = X_train.shape\n",
        "num_test, _ = X_test.shape\n",
        "num_classes = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQBq88m_m_u9"
      },
      "source": [
        "Now implement the function `project_data`. \n",
        "\n",
        "This function will take in `data`, and project it onto the basis given by `proj_basis`. `mean` is an optional parameter to be passed in in order to center our `data` in the original space before projection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz7iEYwUi18O"
      },
      "source": [
        "## Implement the function project_data\n",
        "## You want to return the output: proj_data\n",
        "def project_data(data, proj_basis, mean = None, plot = False, labels = None, s = 5, c = None):\n",
        "  \"\"\" Projects the data (adjusted by the mean) onto proj_basis into basis vectors \n",
        "    Inputs: data is of shape N x d\n",
        "            proj_basis is of shape k x d\n",
        "            mean is of shape d x 1 (optional)\n",
        "            plot: true/false, whether the plot the projections. (optional)\n",
        "            labels: pass in the label of each data to color when plotting.\n",
        "            s, c: size and color, extra arguments to help with plotting. \n",
        "    Outputs: proj_data - the data projected onto proj_basis.\n",
        "    \"\"\"\n",
        "  if mean is not None:\n",
        "    data = data - mean\n",
        "\n",
        "  ## CODE HERE\n",
        "\n",
        "\n",
        "  ## END CODE\n",
        "  if plot:\n",
        "    if labels is not None:\n",
        "\n",
        "      for i, l in enumerate(np.unique(labels)):\n",
        "        if c is not None: plt.scatter(proj_data[labels == l,0], proj_data[labels == l ,1], label = \"class: \" + str(l), s = s, c = c)\n",
        "        else: plt.scatter(proj_data[labels == l,0], proj_data[labels == l ,1], label = \"class: \" + str(l), s = s)\n",
        "        plt.legend();\n",
        "    else: \n",
        "      if c is None: plt.scatter(proj_data[:,0],proj_data[:,1], s = s)\n",
        "      else:  plt.scatter(proj_data[:,0],proj_data[:,1], s = s, c = c)\n",
        "\n",
        "  return proj_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjtq3n_cnmcW"
      },
      "source": [
        "Now let us see an example of our waveforms projected onto two random coordinated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETXl42ZphIMo"
      },
      "source": [
        "# project onto 2 random coordinated\n",
        "coord = np.random.rand(2, dims);\n",
        "plt.figure(figsize = (7,5))\n",
        "project_data(X_train, coord, mean = X_train.mean(axis=0), plot = True, labels = y_train);\n",
        "plt.title('Random Projections');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5haoKxfTBI"
      },
      "source": [
        "def train_PCA(data, n_components):\n",
        "  \"\"\" This function will train your PCA projection by identifying a new basis for your data.\n",
        "  Inputs: data is Nxd\n",
        "          n_components is number of basis you want to reduce the dimensions to \n",
        "  Note: make sure you subtract the mean before you perform svd\n",
        "  Outputs:\n",
        "          basis: the basis identified by PCA\n",
        "          mean: mean in the original space.\n",
        "  \"\"\"\n",
        "  ### CODE HERE\n",
        "  \n",
        "  ### CODE END\n",
        "  return basis, mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOHB_SCoGUL"
      },
      "source": [
        "Now apply `train_PCA` to the waveform data and visualize your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM8QMs_YfS-h"
      },
      "source": [
        "## Train your PCA implementation on X_train, and obtain your basis vectors. Plot out at least 2 basis vectors.\n",
        "# Put the mean spike signal in all_mean\n",
        "n_bases = 3\n",
        "\n",
        "## CODE HERE\n",
        "\n",
        "## CODE END\n",
        "\n",
        "plt.figure(figsize = (6,5))\n",
        "plt.plot(all_mean, linewidth = 5); plt.title('Mean Spike'); plt.show();\n",
        "\n",
        "## Plot the basis vectors\n",
        "## CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9-6ShifoUHA"
      },
      "source": [
        "Now that you have your PCA identified basis vectors, let us view the projections of the spike waveforms in that basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKfzIT2ndsIn"
      },
      "source": [
        "plt.figure(figsize = (7,5))\n",
        "proj_train = project_data(X_train, nbasis, mean = nmean, plot = True, labels = y_train);\n",
        "plt.title('First 2 Basis Projection (Training)');\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "proj_test = project_data(X_test, nbasis, mean = nmean, plot = True, labels = y_test);\n",
        "plt.title('First 2 Basis Projection (Testing)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgLsZFU588_"
      },
      "source": [
        "# We can visualize the projection in 3 dimensions if your PCA returned more than 2 basis vectors.\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "if n_bases > 2:\n",
        "  fig=plt.figure(figsize=(10,7))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  for i in np.arange(num_classes):\n",
        "    Axes3D.scatter(ax, proj_train[y_train==i,0],proj_train[y_train==i,1],proj_train[y_train==i,2], alpha = .4)\n",
        "  plt.title('Training')\n",
        "\n",
        "  fig=plt.figure(figsize=(10,7))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  for i in np.arange(num_classes):\n",
        "    Axes3D.scatter(ax, proj_test[y_test==i,0],proj_test[y_test==i,1],proj_test[y_test==i,2], alpha = .4)\n",
        "  plt.title('Testing')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiF1FAvY5S4Z"
      },
      "source": [
        "Now let us compare with sklearn PCA implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8_exGABbkGD"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_trans = pca.fit_transform(X_train)\n",
        "plt.figure(figsize = (7,5))\n",
        "for i in np.arange(num_classes):\n",
        "  plt.scatter(X_trans[y_train==i,0], X_trans[y_train==i,1], s = 5)\n",
        "plt.title('Training Projection')\n",
        "\n",
        "plt.show();\n",
        "\n",
        "X_trans = pca.transform(X_test)\n",
        "plt.figure(figsize = (7,5))\n",
        "for i in np.arange(num_classes):\n",
        "  plt.scatter(X_trans[y_test==i,0], X_trans[y_test==i,1], s = 5)\n",
        "plt.title('Test Projection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wUh4xA1cM4u"
      },
      "source": [
        "# Spike Sorting: Clustering\n",
        "When we do actual recordings, we do not know the underlying classes of the waveforms - so we will need to perform unsupervised clustering in order to determine the different groups of waveforms. We see that we have three distinct clusters in PCA. As such, we will implement an algorithm called k-means from sklearn which will perform unsupervised clustering in order to extract the cluster labels for you. The algorithm uses an iterative approach to identify centroids of k groups. You will learn more about this algorithm throughout the machine learning course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xbIsZmGcMS5"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(proj_train)\n",
        "kmean_classes = kmeans.labels_\n",
        "\n",
        "fig=plt.figure(figsize=(10,7))\n",
        "for i in np.arange(num_classes):\n",
        "  plt.scatter(proj_train[kmean_classes==i,0],proj_train[kmean_classes==i,1], s = 5)\n",
        "plt.title('K-Means Clusters');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8Xwu7pihxlS"
      },
      "source": [
        "## Now that we identified the 3 clusters, find the centroids.\n",
        "## Hint: the centroids are the average of the points in each cluster.\n",
        "\n",
        "centroid = list()\n",
        "for i in np.arange(3):\n",
        "## CODE HERE\n",
        "  \n",
        "## CODE END\n",
        "\n",
        "fig=plt.figure(figsize=(10,7))\n",
        "for i in np.arange(num_classes):\n",
        "  plt.scatter(proj_train[kmean_classes==i,0],proj_train[kmean_classes==i,1], s = 5)\n",
        "  plt.scatter(centroid[i][0], centroid[i][1], c = 'k', s = 70)\n",
        "plt.title('K-Means Clusters with Centroids');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAO-EVcEpDN5"
      },
      "source": [
        "# Spike Sorting: Classifying New Signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R71OlkZPpG5b"
      },
      "source": [
        "Now that you have projected and clustered your data, you want to be able to identify the class of new spike waveforms. \n",
        "\n",
        "Implement the function `classify_neuron`, where given a new waveform shape, classify the waveform based upon the closest centroid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buI7phsvcEQa"
      },
      "source": [
        "# Implement the function classify_neuron. \n",
        "# You want to compute distances to each centroid for each waveform\n",
        "# Then find the class of the closest centroid to classify your waveform.\n",
        "def classify_neuron(proj_waveform, centroid):\n",
        "  \"\"\"Inputs: proj_waveform: Nxd matrix of projected neuronal spike waveforms\n",
        "             centroid: centroids of your classes\n",
        "     Outputs: classes (Nx1) for each waveform.\n",
        "  \"\"\"\n",
        "  nclass = len(centroid)\n",
        "  distances = np.empty((proj_waveform.shape[0], nclass))\n",
        "  ## CODE HERE\n",
        "\n",
        "  \n",
        "  ## CODE END\n",
        "  return np.argmin(distances, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xeM25HVprGo"
      },
      "source": [
        "proj_test = project_data(X_test, nbasis, nmean)\n",
        "test_classes = classify_neuron(proj_test, centroid)\n",
        "\n",
        "fig=plt.figure(figsize=(10,7))\n",
        "for i in np.arange(num_classes):\n",
        "  plt.scatter(proj_test[test_classes==i,0],proj_test[test_classes==i,1], s = 5)\n",
        "  plt.scatter(centroid[i][0], centroid[i][1], c = 'k', s = 70)\n",
        "plt.title('Classification of Test Neurons with Centroids');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRe54-gOp1E5"
      },
      "source": [
        "# Beyond Spike Sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUohAKZfp5G4"
      },
      "source": [
        "Congratulations! Now you have performed spike sorting, clustered neuronal signal, and performed classification on new signals. The next few blocks show you a few analytics you can extract from spike sorting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdNR3viBqGti"
      },
      "source": [
        "You can count the number of times that each neuron fired in a time frame..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt886xi1tVOA"
      },
      "source": [
        "# Count number of times single neurons fire in the full signal\n",
        "proj_all = project_data(waveforms, nbasis, nmean)\n",
        "classes_all = classify_neuron(proj_all, centroid)\n",
        "\n",
        "num_firings = np.unique(classes_all, return_counts = True)[1]\n",
        "\n",
        "print('Neuron 1 fired {} times'.format(num_firings[1]))\n",
        "print('Neuron 2 fired {} times'.format(num_firings[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHyeLe-0qOMn"
      },
      "source": [
        "You can look at interspike intervals - this tells you the frequency of a neuron's activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xfcoyZ_ykNY"
      },
      "source": [
        "# look at interspike intervals for each cluster!\n",
        "for i in np.arange(num_classes):\n",
        "  times = spiketimes[classes_all == i]\n",
        "  time_diff = np.diff(times)/fs\n",
        "  plt.figure(figsize = (15,3))\n",
        "  plt.hist(time_diff, bins = 50); plt.ylabel('Inter-spike interval (seconds)')\n",
        "  plt.xlim([0, 2]); plt.title('Neuron cluster '+ str(i)); plt.ylabel('Counts')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DaXSAX4qVBk"
      },
      "source": [
        "You can also look at the spike trains - which are the spikes of each neuron over time, and examine its activity over time!. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZqkvih5sM3E"
      },
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "for i in np.arange(3):\n",
        "  plt.eventplot(spiketimes[classes_all == i], lineoffsets = -i);\n",
        "plt.xlim([0,200000]); plt.yticks([0, -1, -2], ['Ensemble (Cluster 1)','Neuron 1 (Cluster 2)','Neuron 2 (Cluster 3)']);\n",
        "plt.xticks(np.linspace(0, 200000, 9), [\"{:.2f}\".format(x) for x in np.linspace(0, 200000/fs, 9)]); plt.xlabel('Seconds');\n",
        "plt.title('Spike Train');"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}