{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MinNorm-student.ipynb","provenance":[{"file_id":"1iAomGTuLfgu3zVpEf4wbWKZehf88aJ4F","timestamp":1606029033711}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"URWy0ZyU9nRe"},"source":["# SVD review"]},{"cell_type":"markdown","metadata":{"id":"Y1jLssdAkuC2"},"source":["**Singular Value Decomposition(SVD)** \n","is a way of factorizing any $m\\times n$ matrix $A$into singular vectors and singular values. SVD is more general than eigendecomposition, but can give us similar information. \n","\n","The SVD of a matrix A can be written as: \n","\n","$$\\color{Blue}{A = UDV^{\\top} \\tag{24}}$$\n","\n","Where $U$ is $m\\times m$ orthonormal, $D$ is $m \\times n$ diagonal matrix with non-negative singular values on the diagonals, and $V^T$ is $n \\times n$ orthonormal. \n","$(U^{\\top} = U^{-1} \\ \\text{and} \\ V^{\\top} = V^{-1})$\n","\n","The columns of $U$ are known as the **left-singular vectors**. The columns of $V$ are known as the **right-singular vectors**.\n","\n","Geometrically, $U$ and $V$ can be thought of as rotation matrices, while $D$ is a scaling and projection matrix. \n","\n","![Singular Value Decomposition](https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0208a.png)"]},{"cell_type":"markdown","metadata":{"id":"jA-6znYt9qS0"},"source":["# Moore Penrose Pseudoinverse"]},{"cell_type":"markdown","metadata":{"id":"s1YlvoTNWBAC"},"source":["When solving linear least squares problems in the form $Xw = y$ ($X$ data matrix, $w$ coefficients, $y$ observations to predict). Often, $X$ is not square or directly invertible. \n","\n","The **Moore-Penrose pseudoinverse** $X^+$gives us a generalization of the inverse matrix that can help us solve linear least squares solutions, $w = X^+ y$ . \n","\n","A computationally simple and accurate of computing the pseudoinverse involves using the singular value decomposition. If $X = UDV^T$, then the pseudoinverse would be $$X^+ = VD^+ U^T$$ where $D^+$ of a diagonal matrix $D$ is obtained by taking the reciprocal of the nonzero diagonal entries, then transposing that matrix. \n","\n","Let us go through an example:"]},{"cell_type":"code","metadata":{"id":"dJfMx6tTgfVM"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os \n","%matplotlib inline\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJELvXYKYLPr"},"source":["matrix_X = np.random.uniform(size = [3, 2], low=1, high=10)\n","print(\"Matrix X: \\n{}\\n\".format(matrix_X));\n","plt.imshow(matrix_X, cmap = 'Reds',vmin = 1, vmax = 10); plt.title(r'$X$');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gWkaNtlY3Yx"},"source":["## compute the svd of the matrix X. You can use numpy.\n","## CODE HERE\n","\n","## CODE END\n","\n","print(\"Matrix U: \\n{} \\n\\nMatrix D: \\n{} \\n\\nMatrix V^T: \\n{}\\n\".format(u, d, vt))\n","\n","fig, ax = plt.subplots(1,3, figsize = (10,3))\n","ax[0].imshow(u, cmap = 'Blues', aspect = 'auto'); ax[0].set_title('U')\n","ax[1].imshow(np.concatenate((np.diag(d),np.zeros([1,2]))), cmap = 'Greys', vmin = 0); \n","ax[1].set_title('D')\n","ax[2].imshow(vt, cmap = 'Reds'); ax[2].set_title('Vt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7dhapl0cZoq"},"source":["## compute the pseudo-inverse of matrix D\n","## remember, you keep the nonzero singular values, then transpose the resulting matrix\n","## CODE HERE\n","dplus = \n","## CODE END\n","print(\"D plus: \\n{}\\n\".format(dplus))\n","\n","\n","## Then compute the pseudo-inverse of matrix X\n","## CODE HERE\n","Xplus = \n","## CODE END\n","print(\"The Moore-Penrose pseudoinverse of Matrix X: \\n{}\".format(Xplus))\n","\n","plt.imshow(matrix_X, cmap = 'Reds',vmin = 1, vmax = 10); plt.title(r'$X$');plt.show()\n","plt.imshow(Xplus,cmap = 'Reds'); plt.title(r'$X^+$'); plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-axmWGqWqFQ4"},"source":["# Double check with numpy's pseudoinverse function\n","np.linalg.pinv(matrix_X)\n","plt.imshow(np.linalg.pinv(matrix_X), cmap = 'Reds'); plt.title(r'$X^+$'); plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJn6I7ZVfJTH"},"source":["# Minimum Norm"]},{"cell_type":"markdown","metadata":{"id":"1DsSU7AufMuz"},"source":["When solving a system of linear equations of the form $Xw = y$, if X is full rank we have 3 scenerios:\n","\n","1.  X is square. In this case, we can find $w = X^{-1}y$\n","\n","2.  X is tall. In this case, we have an overdetermined system. We can find the least squares solution by minimizing the error or residual $\\Vert Xw - y \\Vert^2$\n","$$w = (X^T X)^{-1}X^T y = X^+ y$$ where we have the pseudo-inverse $$X^+ = (X^T X)^{-1}X^T $$\n","\n","3. X is wide. In this case, we have an underdetermined system, and there are infinitely many solutions that can solve $Xw = y$.\n","In this scenerio, we are interested in the **minimum norm** solution:\n","$$\\min_{w} \\left \\| w \\right \\| ^2 \\text{ s.t. } Xw = y$$\n","In lecture, we showed that the pseudo-inverse gives us our minimum norm solution:\n","$$w = X^T(X X^T)^{-1} y = X^+ y$$\n","where we have pseudo-inverse\n","$$X^+ = X^T(X X^T)^{-1} $$"]},{"cell_type":"markdown","metadata":{"id":"HXIfyA90hyce"},"source":["## Underdetermined System of Equations\n","Let us have some practice solving underdetermined systems.\n","\n","Starting with the simplest case: n = 1, d = 2. (one point, 2 dimensions).\n","\n","We want to solve $Xw = y$ for $w$, where $X = [2, 1]$ and $y = 1$. \n","Below, we see that the black dot represents our one point, but since our problem is underdetermined, we have infinite solutions for $w$! A couple solution are represented as the lines in the figure below. \n","\n"]},{"cell_type":"code","metadata":{"id":"kmdiR2GxhiKF"},"source":["X = np.array([2,1])\n","print('X: [x1 1] : ', X)\n","y = 1\n","print('y = ',y)\n","print('We want the solution to [{}]*w = {}'.format(X,y))\n","plt.scatter(X[0],X[1], zorder = 5, linewidths = 2, c = 'k')\n","\n","x_range = np.linspace(-1,4,10)\n","y1 = x_range*1/2\n","plt.plot(x_range, y1, c = 'r', label = 'line1')\n","y2 = x_range*2-3\n","plt.plot(x_range, y2, c = 'g', label = 'line2')\n","y3 = x_range*-2+5\n","plt.plot(x_range, y3, c = 'c', label = 'line3')\n","y4 = x_range*2\n","plt.plot([2]*10,y4, c = 'orange', label = 'line4')\n","plt.legend()\n","plt.title('Many solutions for $w$ solving $Xw = y$')\n","\n","X = np.expand_dims(X, axis = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZpzLa8tmVCn"},"source":["## Compute the minimum norm solution for w. You can use the pseudo-inverse\n","## Your solution for w should be in wstar\n","## Start Code\n","\n","## End code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jATt4dnAsYRB"},"source":["plt.scatter(2,1, zorder = 5, linewidths = 2, c = 'k')\n","\n","x_lines = np.vstack((x_range, np.ones(x_range.shape))).T\n","ys = x_lines@wstar\n","plt.plot(x_range, ys, label = 'minimum norm solution')\n","plt.legend()\n","print('Equation: 1 = X*w = x1*{} + {}'.format(wstar[0],wstar[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXJjz5JHrzEX"},"source":["## Pseudoinverse to fit Overdetermined system"]},{"cell_type":"markdown","metadata":{"id":"4WJ8rHYFvveQ"},"source":["We can use pseudoinverses to fit many points. \n","With a set of x and y, we want to find the line y = mx+b that minimises the error between the fit and actual data points.\n","$$\\min_w ||Xw-y||$$"]},{"cell_type":"markdown","metadata":{"id":"A7FoxK8PkmwW"},"source":["You may remember this problem formulation when you learned about Ordinary Least Squares (OLS), and you can solve this problem using the OLS solution. Below, we will see that the pseudo-inverse also gives us the solution to this problem.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"iPj47Hnbr0po"},"source":["Let us use this system of equation: \n","$$\\boldsymbol{Xw} = \\boldsymbol{y} \\Leftrightarrow \\begin{bmatrix} 0 & 1 \\\\\\\\ 1 & 1 \\\\\\\\ 2 & 1 \\\\\\\\ 3 & 1 \\\\\\\\ 3 & 1 \\\\\\\\ 4 & 1 \\end{bmatrix} \\begin{bmatrix} m \\\\\\\\ b \\end{bmatrix} = \\begin{bmatrix} 2 \\\\\\\\ 4 \\\\\\\\ 0 \\\\\\\\ 2 \\\\\\\\ 5 \\\\\\\\ 3 \\end{bmatrix}$$\n","This can also be represented as a system of equations: \n","$$\\begin{cases} 0m + 1b = 2 \\\\\\\\ 1m + 1b = 4 \\\\\\\\ 2m + 1b = 0 \\\\\\\\ 3m + 1b = 2 \\\\\\\\ 3m + 1b = 5 \\\\\\\\ 4m + 1b = 3 \\end{cases}$$"]},{"cell_type":"code","metadata":{"id":"g0bdAchsriod"},"source":["X = np.array([[0, 1], [1, 1], [2, 1], [3, 1], [3, 1], [4, 1]])\n","y = np.array([[2], [4], [0], [2], [5], [3]])\n","plt.scatter(X[:,0],y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzM6Sq8DsCvu"},"source":["## Compute the pseudoinverse of X and find w\n","w_ols = np.linalg.inv(X.T@X)@X.T@y\n","\n","## CODE HERE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btgsL61msUWJ"},"source":["x_range = np.linspace(-1, 5, 1000)\n","ys = w[0]*x_range + w[1] # Your solution\n","y_ols = w_ols[0]*x_range + w_ols[1] # OLS solution\n","\n","fig, ax = plt.subplots(1,2, figsize = (15,5))\n","ax[0].plot(X[:, 0], y, '*')\n","ax[0].plot(x_range, ys); ax[0].set_title('Pseudoinverse Solution')\n","ax[0].set_xlim(-1., 6);ax[0].set_ylim(-0.5, 5.5)\n","\n","ax[1].plot(X[:, 0], y, '*')\n","ax[1].plot(x_range, y_ols); ax[1].set_title('OLS Solution')\n","ax[1].set_xlim(-1., 6); ax[1].set_ylim(-0.5, 5.5)\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}